{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ec69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b41d634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>transformed_question1</th>\n",
       "      <th>transformed_question2</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_num_words</th>\n",
       "      <th>q2_num_words</th>\n",
       "      <th>word_common</th>\n",
       "      <th>word_total</th>\n",
       "      <th>...</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>token_set_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>65</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.916659</td>\n",
       "      <td>0.785709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "      <td>50</td>\n",
       "      <td>87</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.699993</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66</td>\n",
       "      <td>74</td>\n",
       "      <td>63</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "      <td>72</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.285712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder 23 24 math divided 24 23</td>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>75</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>55</td>\n",
       "      <td>47</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  is_duplicate  \\\n",
       "0           0             0   \n",
       "1           1             0   \n",
       "2           2             0   \n",
       "3           3             0   \n",
       "4           4             0   \n",
       "\n",
       "                               transformed_question1  \\\n",
       "0          step step guide invest share market india   \n",
       "1                    story kohinoor koh noor diamond   \n",
       "2       increase speed internet connection using vpn   \n",
       "3                              mentally lonely solve   \n",
       "4  one dissolve water quikly sugar salt methane c...   \n",
       "\n",
       "                               transformed_question2  q1_len  q2_len  \\\n",
       "0                step step guide invest share market      65      56   \n",
       "1  would happen indian government stole kohinoor ...      50      87   \n",
       "2               internet speed increased hacking dns      72      58   \n",
       "3            find remainder 23 24 math divided 24 23      49      58   \n",
       "4                      fish would survive salt water      75      38   \n",
       "\n",
       "   q1_num_words  q2_num_words  word_common  word_total  ...   csc_min  \\\n",
       "0            14            12           11          23  ...  0.999983   \n",
       "1            12            17            8          26  ...  0.749981   \n",
       "2            14            10            4          24  ...  0.399992   \n",
       "3            12            16            1          22  ...  0.000000   \n",
       "4            15             7            4          21  ...  0.999950   \n",
       "\n",
       "    csc_max   ctc_min   ctc_max  last_word_eq  first_word_eq  fuzz_ratio  \\\n",
       "0  0.999983  0.916659  0.785709           0.0            1.0          93   \n",
       "1  0.599988  0.699993  0.466664           0.0            1.0          66   \n",
       "2  0.249997  0.399996  0.285712           0.0            1.0          43   \n",
       "3  0.000000  0.000000  0.000000           0.0            0.0           9   \n",
       "4  0.666644  0.571420  0.307690           0.0            1.0          35   \n",
       "\n",
       "   fuzz_partial_ratio  token_sort_ratio  token_set_ratio  \n",
       "0                 100                93              100  \n",
       "1                  74                63               86  \n",
       "2                  46                63               63  \n",
       "3                  11                25               28  \n",
       "4                  55                47               67  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"transformed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d76c4f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064c396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404287 entries, 0 to 404286\n",
      "Data columns (total 28 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   Unnamed: 0             404287 non-null  int64  \n",
      " 1   id                     404287 non-null  int64  \n",
      " 2   qid1                   404287 non-null  int64  \n",
      " 3   qid2                   404287 non-null  int64  \n",
      " 4   question1              404104 non-null  object \n",
      " 5   question2              404278 non-null  object \n",
      " 6   is_duplicate           404287 non-null  int64  \n",
      " 7   transformed_question1  404099 non-null  object \n",
      " 8   transformed_question2  404211 non-null  object \n",
      " 9   q1_len                 404287 non-null  int64  \n",
      " 10  q2_len                 404287 non-null  int64  \n",
      " 11  q1_num_words           404287 non-null  int64  \n",
      " 12  q2_num_words           404287 non-null  int64  \n",
      " 13  word_common            404287 non-null  int64  \n",
      " 14  word_total             404287 non-null  int64  \n",
      " 15  word_share             404287 non-null  float64\n",
      " 16  cwc_min                404287 non-null  float64\n",
      " 17  cwc_max                404287 non-null  float64\n",
      " 18  csc_min                404287 non-null  float64\n",
      " 19  csc_max                404287 non-null  float64\n",
      " 20  ctc_min                404287 non-null  float64\n",
      " 21  ctc_max                404287 non-null  float64\n",
      " 22  last_word_eq           404287 non-null  float64\n",
      " 23  first_word_eq          404287 non-null  float64\n",
      " 24  fuzz_ratio             404287 non-null  int64  \n",
      " 25  fuzz_partial_ratio     404287 non-null  int64  \n",
      " 26  token_sort_ratio       404287 non-null  int64  \n",
      " 27  token_set_ratio        404287 non-null  int64  \n",
      "dtypes: float64(9), int64(15), object(4)\n",
      "memory usage: 86.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cae133c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                0\n",
       "is_duplicate              0\n",
       "transformed_question1    87\n",
       "transformed_question2    76\n",
       "q1_len                    0\n",
       "q2_len                    0\n",
       "q1_num_words              0\n",
       "q2_num_words              0\n",
       "word_common               0\n",
       "word_total                0\n",
       "word_share                0\n",
       "cwc_min                   0\n",
       "cwc_max                   0\n",
       "csc_min                   0\n",
       "csc_max                   0\n",
       "ctc_min                   0\n",
       "ctc_max                   0\n",
       "last_word_eq              0\n",
       "first_word_eq             0\n",
       "fuzz_ratio                0\n",
       "fuzz_partial_ratio        0\n",
       "token_sort_ratio          0\n",
       "token_set_ratio           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "701fa3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85093a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0               0\n",
       "is_duplicate             0\n",
       "transformed_question1    0\n",
       "transformed_question2    0\n",
       "q1_len                   0\n",
       "q2_len                   0\n",
       "q1_num_words             0\n",
       "q2_num_words             0\n",
       "word_common              0\n",
       "word_total               0\n",
       "word_share               0\n",
       "cwc_min                  0\n",
       "cwc_max                  0\n",
       "csc_min                  0\n",
       "csc_max                  0\n",
       "ctc_min                  0\n",
       "ctc_max                  0\n",
       "last_word_eq             0\n",
       "first_word_eq            0\n",
       "fuzz_ratio               0\n",
       "fuzz_partial_ratio       0\n",
       "token_sort_ratio         0\n",
       "token_set_ratio          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ed56ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404146, 23)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f030d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kisha\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea1e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\kisha\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04074187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84c131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3001cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "badde8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = new_df['transformed_question1'].tolist() + new_df['transformed_question2'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55e5e886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dabba trading\n"
     ]
    }
   ],
   "source": [
    "print(sent[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c322bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# for i in range(0,len(df)):\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "#     review = review.split()\n",
    "#     review = ' '.join(review)\n",
    "#     corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad042f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for sent in corpus:\n",
    "    sent_token=sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296bed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0642a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae934bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a7723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f57227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3be90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2dae60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb86803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f22236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73932d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00908b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4340101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82907076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    255042\n",
      "1    149306\n",
      "Name: is_duplicate, dtype: int64\n",
      "0    63.074876\n",
      "1    36.925124\n",
      "Name: is_duplicate, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df['is_duplicate'].value_counts())\n",
    "print((df['is_duplicate'].value_counts()/df['is_duplicate'].count())*100)\n",
    "df['is_duplicate'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e2843f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Question 789795\n",
      "Number of duplicate question 13698\n"
     ]
    }
   ],
   "source": [
    "qid = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
    "print('Number of Unique Question', np.unique(qid).shape[0])\n",
    "x = qid.value_counts() > 1\n",
    "print('Number of duplicate question',x[x].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "179b4acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(qid.value_counts().values,bins=160)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ff0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.sample(10000,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a4b0305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302477</th>\n",
       "      <td>302477</td>\n",
       "      <td>594007</td>\n",
       "      <td>594008</td>\n",
       "      <td>What are kalman filters used for?</td>\n",
       "      <td>What are examples of filters used in a home?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369857</th>\n",
       "      <td>369857</td>\n",
       "      <td>723787</td>\n",
       "      <td>723788</td>\n",
       "      <td>How do Relationship work?</td>\n",
       "      <td>What matters in the life most dignity or money?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>4665</td>\n",
       "      <td>9325</td>\n",
       "      <td>9326</td>\n",
       "      <td>How would an arbitrageur seek to capitalize gi...</td>\n",
       "      <td>How would an arbitrageur seek to capitalize gi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54203</th>\n",
       "      <td>54203</td>\n",
       "      <td>107861</td>\n",
       "      <td>107862</td>\n",
       "      <td>Why did Quora mark my question as incomplete?</td>\n",
       "      <td>Why does Quora detect my question as an incomp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132567</th>\n",
       "      <td>132567</td>\n",
       "      <td>262555</td>\n",
       "      <td>262556</td>\n",
       "      <td>Does Google Maps account for change in speed w...</td>\n",
       "      <td>What speed does Google Maps assume when it gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "302477  302477  594007  594008   \n",
       "369857  369857  723787  723788   \n",
       "4665      4665    9325    9326   \n",
       "54203    54203  107861  107862   \n",
       "132567  132567  262555  262556   \n",
       "\n",
       "                                                question1  \\\n",
       "302477                  What are kalman filters used for?   \n",
       "369857                          How do Relationship work?   \n",
       "4665    How would an arbitrageur seek to capitalize gi...   \n",
       "54203       Why did Quora mark my question as incomplete?   \n",
       "132567  Does Google Maps account for change in speed w...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "302477       What are examples of filters used in a home?             0  \n",
       "369857    What matters in the life most dignity or money?             0  \n",
       "4665    How would an arbitrageur seek to capitalize gi...             0  \n",
       "54203   Why does Quora detect my question as an incomp...             1  \n",
       "132567  What speed does Google Maps assume when it gen...             1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17b82192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess(q):\n",
    "    \n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    # Decontracting words\n",
    "    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "    # https://stackoverflow.com/a/19794953\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "\n",
    "    \n",
    "    return q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eae5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['question1'] = new_df['question1'].apply(preprocess)\n",
    "new_df['question2'] = new_df['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e3ee335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text_hammer in c:\\users\\kisha\\anaconda3\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: TextBlob in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from text_hammer) (0.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from text_hammer) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from text_hammer) (1.4.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.9.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from text_hammer) (4.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from text_hammer) (3.4.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from beautifulsoup4==4.9.1->text_hammer) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from pandas->text_hammer) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from pandas->text_hammer) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->text_hammer) (1.16.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (0.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (8.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (2.27.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (3.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (61.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (3.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (0.6.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (1.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (1.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from spacy->text_hammer) (2.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy->text_hammer) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy->text_hammer) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy->text_hammer) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy->text_hammer) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->text_hammer) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy->text_hammer) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from jinja2->spacy->text_hammer) (2.0.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from TextBlob->text_hammer) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob->text_hammer) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\kisha\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob->text_hammer) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install text_hammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "501a9a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\kisha\\music\\quora-question-pairs-main\\en_core_web_sm-3.1.0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'en_core_web_sm-3.1.0-py3-none-any.whl' looks like a filename, but the file does not exist\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\kisha\\\\Music\\\\quora-question-pairs-main\\\\en_core_web_sm-3.1.0-py3-none-any.whl'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install en_core_web_sm-3.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f4a403a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtext_hammer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mth\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\text_hammer\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_hammer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tqdm_notebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_notebook\n\u001b[0;32m      3\u001b[0m tqdm_notebook\u001b[38;5;241m.\u001b[39mpandas()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\text_hammer\\utils.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_sm\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m     17\u001b[0m nlp \u001b[38;5;241m=\u001b[39m en_core_web_sm\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "import text_hammer as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e85e3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from tqdm._tqdm_notebook import tqdm_notebook\n",
    "# tqdm_notebook.pandas()\n",
    "\n",
    "# def text_preprocessing(df,col_name):\n",
    "#     column = col_name\n",
    "#     df[column] = df[column].progress_apply(lambda x:str(x).lower())\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) #you're -> you are; i'm -> i am\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_stopwords(x))\n",
    "# #     df[column] = df[column].progress_apply(lambda x:th.spelling_correction(x))\n",
    "  \n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n",
    "#     return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e35d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cb074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990930cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71621ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after tokenization only\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['q1_len'] = new_df['question1'].str.len() \n",
    "new_df['q2_len'] = new_df['question2'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdcae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "new_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664861ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "    return len(w1 & w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['word_common'] = new_df.apply(common_words, axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "    return (len(w1) + len(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['word_total'] = new_df.apply(total_words, axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d328ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Features\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def fetch_token_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    SAFE_DIV = 0.0001 \n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    token_features = [0.0]*8\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    return token_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features = new_df.apply(fetch_token_features, axis=1)\n",
    "\n",
    "new_df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "new_df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "new_df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "new_df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "new_df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "new_df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "new_df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "new_df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "\n",
    "def fetch_length_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    length_features = [0.0]*3\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return length_features\n",
    "    \n",
    "    # Absolute length features\n",
    "    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    #Average Token Length of both Questions\n",
    "    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    \n",
    "    strs = list(distance.lcsubstrings(q1, q2))\n",
    "    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
    "    \n",
    "    return length_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_features = new_df.apply(fetch_length_features, axis=1)\n",
    "\n",
    "new_df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\n",
    "new_df['mean_len'] = list(map(lambda x: x[1], length_features))\n",
    "new_df['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e98c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy Features\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fetch_fuzzy_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    fuzzy_features = [0.0]*4\n",
    "    \n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c835b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_features = new_df.apply(fetch_fuzzy_features, axis=1)\n",
    "\n",
    "# Creating new feature columns for fuzzy features\n",
    "new_df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "new_df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "new_df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "new_df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fa671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f78a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df[['ctc_min', 'cwc_min', 'csc_min', 'is_duplicate']],hue='is_duplicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde246c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777e3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e11c219f",
   "metadata": {},
   "source": [
    "1) drop missing values\n",
    "2) the 'is_duplicate' is biased towards 0\n",
    "3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc6493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd4688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807368b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
